{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>![[logo_with_name.PNG]]</p> <p>Meta-PepView is a metaproteomics dashboard interface designed for evaluation of metaproteomics experiments and analysis of microbial communities. It provides modules to visualize the taxonomic composition and expressed protein functions from microbial community data, as well as modules for evaluating metaproteomics experiment performance. It features a user-friendly, web-based interface that can be run locally on desktop PCs. Meta-PepView enables users to perform large scale cross-sample quantitative analysis of community composition and protein functions, compare results from database searches and de novo annotations within samples, and evaluate the quality and coverage of metaproteomics experiments and reference databases. It is designed to be compatible with a variety of metaproteomics software tools.</p> <p>Main features of meta-PepView include:</p> <ul> <li>Visualization of taxonomic compositions and expressed protein functions from metaproteomics experiments as obtained from various database search or de novo sequencing engines.</li> <li>DB independent taxonomy classification from de novo peptide sequences using the Unipept API and comparison of the obtained de novo taxonomic composition with DB search results.</li> <li>Analysis of mass spectrometric performance from mzML raw files and evaluation of DB search and de novo matches from spectral data.</li> <li>Benchmarking of experimental performance with a reference dataset, consisting of a large number of reference experiments.</li> </ul>"},{"location":"#instrument-support","title":"Instrument support","text":"<p>The current version of meta-PepView is designed and has been tested for mass spectrometric raw data obtained from QE Orbitrap mass spectrometers, running in data-dependent acquisition (DDA) mode. Nevertheless, a broad support across mass spectrometry instruments is expected for taxonomic and functional analysis of communities from metaproteomics data. However, the performance evaluation module is not designed for data-independent acquisition (DIA) or PASEF based analysis strategies.</p>"},{"location":"#pc-requirements","title":"PC requirements","text":"<ul> <li>The dashboard is cross-platform and has been tested in Windows 10/11 and Linux.</li> <li>The dashboard can be installed with the python package manager <code>pip</code>, or setup with Docker (or another OCI compliant container manager) via the provided Docker image. See Installation for more information.</li> <li>A minimum of 16 GB RAM is required for full operation of the dashboard, but 32 GB is recommended.</li> <li>The dashboard can be accessed through both Firefox (Gecko browser engine) and any Chromium based browser (Chrome, Edge, Brave, etc.).</li> <li>Internet connection is required to download taxonomy and function datasets, and for connection to the Unipept API for de novo taxonomy classification.</li> <li>Meta-PepView requires a recent Python version, make sure Python 3.11 or later is installed</li> </ul> <p>Note</p> <p>The browser engines support file import up to a certain threshold: (Chromium: ~300 MB, Firefox: ~1GB). Any larger file (which is common in mzML) will not be imported. Trying to import a too large file into the dashboard will give no response from the browser, no warning will be shown. To enable large file import, it is recommended to compress the files in .zip or .tar.gz format prior to import. If the mzML file is still too large, smaller mzML files could be generated by setting noise filters (top N signal threshold for example).</p>"},{"location":"annotation-pipeline/","title":"Annotation pipeline","text":""},{"location":"annotation-pipeline/#technical-overview-annotation-pipeline","title":"Technical overview annotation pipeline","text":"<p>This section provides a high level overview of the processing steps performed during import and annotation of metaproteomics data. Here, details regarding aggregation and quantification of metaproteomics data is outlined. Several decisions were made in missing data handling and format restrictions to manage potential identification and quantification biases from the datasets. The sections outlined below loosely correspond to functions executed in the dashboard backend. Specific processing steps executed during import depends on user specified annotation options and sources that are included into the dashboard (Merge DB search files into one sample, perform Unipept annotation of peptide sequences, etc.).</p>"},{"location":"annotation-pipeline/#annotation-start-load-dataset-and-set-annotation-options","title":"Annotation start: Load dataset and set annotation options","text":"<ol> <li>Check presence of datasets, are datasets present to do annotation? (see scenarios)</li> <li>Load current sample table, Check and filter DB search data in input dataset and current sample table, also, if sample names have duplicates, add suffix to the new import to separate them.</li> <li>Perform peptide Annotation to process data and expand the sample table.</li> <li>Concatenate <code>MetaPepTable</code> object for new sample with the current sample table.</li> </ol>"},{"location":"annotation-pipeline/#annotate-peptides","title":"Annotate peptides","text":"<ol> <li>Import taxonomy db (if GTDB chosen, do not perform Unipept search)</li> <li>Import taxonomy map file (if uploaded) (perform LCA if protein ID present multiple times)</li> <li>Import function map file (if uploaded)</li> <li>Load cRAP dataset if specified by user</li> <li>Import de novo data file(s) into MetaPepDeNovo format<ol> <li>Create a dictionary that maps: <code>{raw_spectral_name: MetaPepDeNovo_obj}</code></li> <li>When importing and processingde novo filter out crap peptides if specified</li> <li>Add de novo data only if its <code>raw_spectral_name</code> is not present in the dict. Thus, ensure that every spectrum file has only one de novo file within a single sample. (between samples, duplicate files are allowed)</li> </ol> </li> <li>Import database search file(s) if present<ol> <li>If Merge DB search is True (all DB search files are one sample):<ol> <li>Load all DB search files to <code>MetaPepDbSearch</code> (Load metapep DB search) format and store in list, filter out crap peptides if specified for all files</li> <li>Concatenate <code>MetaPepDbSearch</code> tables (one per DB search file)</li> <li>Check during concatenation that source files (raw spectrum file) are unique across objects, otherwise, peptides may be counted double</li> <li>Create new sample dataset for concatenated <code>MetaPepDbSearch</code> file.</li> </ol> </li> <li>If Merge DB search is False (each DB search file is its own sample):<ol> <li>loop through DB search files:<ol> <li>Load an DB search file and process into <code>MetaPepDbSearch</code> (see Load metapep DB search) (filter crap if specified)</li> <li>Create new sample dataset for single <code>MetaPepDbSearch</code> file.</li> <li>Append <code>MetaPepTable</code> into list and iterate to next DB search file.</li> </ol> </li> <li>Concatenate all <code>MetaPepTable</code> objects into single object..</li> </ol> </li> </ol> </li> <li>If no DB search files supplied but only de novo file: Build <code>MetaPepTable</code> (see <code>build_metapep_table()</code> level) for sample with only de novo data and taxonomy mapping. Append sample to existing <code>MetaPepTable</code> and return it.</li> </ol>"},{"location":"annotation-pipeline/#convert-source-specific-metaproteomics-format-into-meta-pepview-format","title":"Convert source specific metaproteomics format into meta-PepView format","text":"<ol> <li>Load correct DB search class based on DB search/de novo format (e.g. Sage).</li> <li>Read data in class and add sample name to it.</li> <li>Convert format specific DB search/de novo object to <code>MetaPep{DbSearch | DeNovo}</code> object.<ol> <li>Rename columns to consistent format (save confidence format as variable).</li> <li>Extract aa sequence from peptides (equate Leucin-Isoleucine, remove PTM).</li> <li>Filter cRAP peptides out.</li> <li>Extract all unique source file names to store in a list in the  <code>MetaPep{DbSearch | DeNovo}</code> object.</li> <li>remove file type suffix from source file, format protein ID delimiter.</li> </ol> </li> </ol>"},{"location":"annotation-pipeline/#process-data-into-new-sample","title":"Process data into new sample","text":"<ol> <li>If DB search data supplied:<ol> <li>Apply confidence threshold cutoff and aggregate spectrum matches to peptide sequence groups: Create PSM column that counts number of observations of peptide sequence, sum MS1 precursor signal intensities, store maximum spectrum confidence as peptide sequence confidence. From the highest confidence scan, take the spectrum information as peptide information (e.g. retention time, m/z, ppm, scan number, etc.)</li> <li><code>taxonomic_annotation()</code> (if protein-taxonomy map present): supplement peptide grouped data with taxonomy ID and lineage information. If no mapping file present, add empty columns instead. If multiple proteins matched against peptide, store LCA of protein taxa.</li> <li><code>functional_annotation()</code> (if protein-function map present): supplement peptide data with KEGG KO information from function mapper. If multiple proteins mapped against peptide: either, only store information if no conflict in ID between proteins (empty values are ignored), or concatenate IDs into a combined string.</li> </ol> </li> <li>If de novo data supplied:<ol> <li><code>include_de_novo()</code>: <ol> <li>If all DB search files are one single sample, take all de novo file data and concatenate to single <code>MetaPepDeNovo</code> (all de novo peptides are included in the sample, no matter the spectrum file source)</li> <li>If each DB search is separate sample: fetch de novo files that match to source files in DB search data files, concatenate only these files (one DB search file is one sample, this file will only match de novo files that come from the same MS runs as that DB search file)</li> <li><code>process_de_novo_data()</code>: Apply confidence cutoff, peptide length cutoff and group spectra in the concatenated de novo object by peptide sequence and sample name. (see <code>process_db_search_data()</code> for aggregation rules)</li> <li><code>merge_de_novo_db_search()</code>: add de novo fields to DB search peptide dataset, match by peptide sequence.</li> <li>Store de novo metadata (confidence format, import status, de novo file format)</li> </ol> </li> </ol> </li> <li>If Unipept taxonomy selected<ol> <li><code>global_taxonomic_annotation()</code>:</li> </ol> </li> <li>Set metadata fields: formats, what data imported, etc.</li> <li>Combine peptide dataset with metadata into <code>MetaPepTable</code> object</li> </ol>"},{"location":"annotation/","title":"Processing and annotation for the compositions and functions dataset","text":"<p>After import data is provided for a new sample in the compositions and functions dataset, import of the sample into the dataset is started by pressing the Add sample button. This may be an resource intensive task depending on the size of the input data and may take a minute to execute. During import, meta-PepView performs the following processing tasks:</p> <ul> <li>Wrangling the input data into a consistent format that is workable for meta-PepView to visualize.</li> <li>Grouping and quantifying peptides from scan counts and signal intensities. Grouping is done per sample.</li> <li>Checking compatibility of the new sample import sources with those present in the project table.</li> <li>Annotating DB search and de novo metaproteomics data with taxonomy information and functional information from the supplied datasets.</li> <li>Update the project table with the new sample data.</li> </ul> <p>A high level overview of the importer and annotation pipeline executed during sample import is provided here. </p>"},{"location":"build-reference-dataset/","title":"Build reference dataset","text":"<p>Meta-PepView provides a few pre-built reference datasets to benchmark experiments against. However, it is recommended to create an in-house reference database consisting of comparable metaproteomics experiments i.e., samples obtained from similar types of biomaterial, as well as consistent analytical instruments. Meta-PepView provides a command line utility mpv-buildref, that allows construction of custom reference datasets for import into the dashboard. This utility extracts metrics from the spectral and (meta)proteomics datasets to provide a consise dataset for the complete range of experiments.</p> <p>mpv-buildref only requires a single root directory location from the user, as well as a few options to manage formatting. mpv-buildref will parse all files inside the provided root directory, collect all relevant datasets, and combine all datasets into their respective experiments.</p>"},{"location":"build-reference-dataset/#running-mpv-buildref","title":"Running mpv-buildref","text":"<p>The mpv-buildref tool is available both in the Docker version, as well as when installed with <code>pip</code>/<code>pipx</code>. For ease of use, it is recommended to run it from a <code>pipx</code> installation, as it will be available directly to the command line:</p> <pre><code>$ mpv-buildref -d peaks11 -n peaks11 -o output.json *root_dir*\n</code></pre> <p>When running inside Docker, the root directory containing the experiments needs to be mounted to the container file system. Then, the container mount point can be called as the root container when calling mpv-buildref. In addition, the output location should be inside a directory (in the container), that is mounted to a host direcory. This may be a separate mount point, or the output location may be inside the root container provided as input:</p> <p>After Creating the image: <pre><code>$ docker run -it -v \"/host/path/to/root_dir:/home/root_dir\" metapepview mpv-buildref -d peaks11 -n peaks11 -o /home/root_dir/output.json */home/root_dir*\n</code></pre></p> <p>To get a description of the tool and its options, run it with the <code>--help</code> option:</p> <pre><code>$ mpv-buildref --help\n\nusage: buildref [-h] -d {peaks10,peaks11,maxquant,sage} \n                -n {peaks10,peaks11,novor,casanovo} [-o OUTPUT] \n                [-D DB_SEARCH_THRESHOLDS] [-N DE_NOVO_THRESHOLDS] \n                [-I INTENSITY_PERCENTILES] [-T TRANSMISSION_LOSS_PERCENTILES]\n                [-r DB_SEARCH_FILE_PATTERN] [-x DE_NOVO_FILE_PATTERN] directory\n\nCreate benchmark dataset of metaproteomics experiments from set of experimental \ndata. It parses a supplied root directory for all relevant experimental data \nfiles (mzML, peptide identification data from db search and de novo). All files \nare matched by their common source file name and performance metrics are \nextracted from the data to construct a performance statistics dataset.\n\npositional arguments:\n  directory\n\noptions:\n  -h, --help            show this help message and exit\n  -d, --db-search-format {peaks10,peaks11,maxquant,sage}\n  -n, --de-novo-format {peaks10,peaks11,novor,casanovo}\n  -o, --output OUTPUT\n  -D, --db-search-thresholds DB_SEARCH_THRESHOLDS\n                        Threshold values used when splitting db search matches \n                        into confidence brackets.\n  -N, --de-novo-thresholds DE_NOVO_THRESHOLDS\n                        Threshold values used when splitting de novo \n                        identifications into confidence brackets.\n  -I, --intensity-percentiles INTENSITY_PERCENTILES\n                        MS signal intensity percentiles to be calculated.\n  -T, --transmission-loss-percentiles TRANSMISSION_LOSS_PERCENTILES\n                        Ion transmission loss percentiles to be calculated.\n  -r, --db-search-file-pattern DB_SEARCH_FILE_PATTERN\n                        Custom regex pattern used to fetch db search match files \n                        by file name.\n  -x, --de-novo-file-pattern DE_NOVO_FILE_PATTERN\n                        Custom regex pattern used to fetch de novo \n                        identification files by file name.\n</code></pre> <p>For correct parsing of proteomics datasets, it is recommended to take the following into account:</p> <ul> <li> <p>To build a complete dataset, provide for all experiments a spectral file (<code>mzML</code>), featuremap (<code>featureXML</code>), as well as DB search and de novo results.</p> </li> <li> <p>When converting raw file names to mzML format. Do not change the name of the mzML file, and ensure that the mzML file name is identical to the raw file name (minus the file type suffix). For each experiment, all data files are automatically linked to each other based on the name of the source file. Depending on the software used for DB search/de novo analysis, this will either be the raw spectral data, or the name of the mzML. In this case, meta-PepView will assume that the name of the raw file and mzML file is the same.</p> </li> <li> <p>Ensure that all DB search and de novo datasets can be recognized as such from the file names (for example, adding a fixed prefix / suffix to each file). If all experiment results were exported using default filenames provided by the analysis software, the tool should be able to automatically recognize all files inside the supplied root directory. However, if custom file names were assigned, regular expression patterns may need to be supplied for the tool to recognize the datasets (DB search: <code>-r</code>, de novo: <code>-x</code>). If no patterns are supplied, it will search DB search and de novo datasets using the following patterns:</p> <p>DB search:</p> <ul> <li>peaks11:   \"db.psms.csv\"</li> <li>peaks10:   \"DB search psm.csv\"</li> <li>maxquant:   \"evidence.txt\"</li> <li>sage':       \"*.sage.tsv\"</li> </ul> <p>de novo</p> <ul> <li>peaks11:   \"*.denovo.csv\"</li> <li>peaks10:   \"de novo peptides.csv\"</li> <li>novor:      \"*.novor.csv\"</li> <li>casanovo:   \"*.mztab\"</li> </ul> </li> <li> <p>It is recommended to set reasonable confidence thresholds for the used DB search (option: <code>-D</code>) and de novo (option: <code>-N</code>) output format. If no thresholds are given, it will default to the following values: DB search: \"30 50 80\", de novo: \"50 80 90\". While many proteomics tools score identifications in a 0 - 100 range, some use a different range. For example, Casanovo scores matches between -1 - 1. For such cases, custom threshold values should be provided to partition peptides in confidence groups.</p> </li> </ul>"},{"location":"community-visualization/","title":"Microbial community visualization","text":"<p>Meta-PepView provides modules to visualize microbial communities from metaproteomics data. Visualization of community composition and expressed protein functions from multiple samples are provided from the Compositions and functions dataset. The following community analysis modules are provided:</p> <ul> <li> <p>Community composition: Visualize taxonomy composition and compare taxonomy between multiple samples</p> </li> <li> <p>Community function: Visualize expression of KEGG terms and compare expression between multiple samples</p> </li> </ul> <p> Community analysis modules highlighted in the sidebar</p>"},{"location":"data-import/","title":"Sample data import","text":"<p>A project consists of two datasets: A spectral dataset from a single experiment run (the Performance evaluation dataset), and a metaproteomics dataset with taxonomy and functional annotations of multiple samples (the Compositions and functions dataset). The Performance evaluation dataset consists of spectral data (mzML), feature data (featureXML), and metaproteomics data (peptide sequences from DB search and de novo identification). Due to the size of the spectral data, only one experimental sample may be stored inside the spectral dataset.</p> <p>The Compositions and functions dataset consists of a dataset of multiple samples, where each sample contains metaproteomics data (peptide sequences from DB search and de novo identification), as well as taxonomy and functional information for each peptide. Generally, a \"sample\" relates to the input sample (metaproteome) used for MS analysis and it may contain peptide data from multiple MS runs (in case of replicates or fractions).</p> <p>To extend an existing project or start a new one, new samples can be imported into the meta-PepView dashboard. This is done in the Create project page. Here, new samples are added from the importer module:</p> <p></p> <p>For each data component, meta-PepView supports input formats from various sources. The Prepare input data section contains a detailed overview of the input formats that is expected by meta-PepView.</p> <p>This page highlights considerations for importing and combining multiple input sources into samples to import into the project. It gives an overview of all the import components, as well as instructions how to combine the sources into a new sample to import.</p>"},{"location":"data-import/#import-components","title":"Import components","text":""},{"location":"data-import/#db-search","title":"DB search","text":"<p>Metaproteomics data from DB search matching of mass spectrometry data is imported in the DB search field. This field expects a dataset of peptide sequence identified scans (one row corresponds to a single spectrum match). Meta-PepView supports several DB search output formats:</p> <ul> <li>Peaks Studio 10/11</li> <li>MaxQuant</li> <li>Sage</li> </ul> <p>More information about the expected input format can be found here.</p> <p>DB search data is a component of both the Performance evaluation dataset and Compositions and functions dataset. When importing DB search data into the Performance evaluation dataset, only a single file may be uploaded and no additional settings are provided outside of the DB search format.</p> <p>When importing DB search data into a new sample for the Compositions and functions dataset, several options are provided: Either a single DB search file may be uploaded, or multiple files may be supplied. If merge DB search files is toggled on, all files are merged and processed as a single sample. If it is toggled off, each DB search file will be processed as a separate sample. </p> <p>In the Options menu (only present for Compositions and functions dataset import), a match confidence cutoff may be specified (format depends on DB search format). In addition, a protein ID pattern may be specified to extract from the protein ID fields and make it compatible with Protein ID report formats from other sources. Finally, a pre-filtering of peptide sequences may be performed based on presence in the cRAP database. </p>"},{"location":"data-import/#de-novo","title":"De novo","text":"<p>Metaproteomics data from de novo peptide identification of DDA mass spectometry data is imported in the De novo field. Similarly to DB search import, this field expects a dataset of peptide sequence identified scans (one row corresponds to a single spectrum identification). Meta-PepView supports several de novo output formats:</p> <ul> <li>Peaks Studio 10/11</li> <li>Novor (SearchGUI)</li> <li>Casanovo</li> </ul> <p>More information about the expected input format can be found here.</p> <p>de novo data is a component of both the Performance evaluation dataset and Compositions and functions dataset. When importing de novo data into the Performance evaluation dataset, only a single file may be uploaded and no additional settings are provided outside of the de novo format.</p> <p>When importing de novo data into a new sample for the Compositions and functions dataset, several options are provided: Either a single de novo file may be uploaded, or multiple files may be supplied. If merge DB search files is toggled on, all de novo files will be used to supplement the sample dataset together with all DB search data. They will be merged and processed as a single sample. If merge DB search files is toggled off, then each de novo file will be matched to a DB search file, based on shared raw file names (originating from the same MS run), and processed separately with the DB search data to form a separate sample for each DB search file. De novo files that do not match to any DB search dataset are not processed. </p> <p>Info</p> <p>In most input formats, a single DB search or de novo file may contain multiple raw file sources. For example, the database search engine used for spectrum matching may combine multiple sample fractions in a single DB search file. The same is true for de novo data. In meta-PepView, if DB search files are not merged (each file is a separate sample), supplementing de novo data to each DB search file sample is done by matching raw file sources with loose criteria. This means that when one shared raw file source is found between a DB search file and a de novo file, they are matched. As a result, one DB search file may be matched to multiple de novo files if it contains multiple raw file sources.</p> <p>In the Options menu (only present for Compositions and functions dataset import), a identification confidence cutoff may be specified (format depends on de novo format). Also, a pre-filtering of peptide sequences may be performed based on presence in the cRAP database. </p> <p>Warning</p> <p>Setting an appropriate confidence cutoff is essential for de novo identification data, but not trivial. One challenge is that there is no reliable strategy for false discovery rate estimation. Also, only a small fraction of mass spectrometry scans contain the full information to identify a full peptide sequence. However, most de novo identification engines will attempt to resolve a peptide sequence from incomplete spectral profiles, resulting in a large fraction of incorrect sequences.</p>"},{"location":"data-import/#spectral-file","title":"Spectral file","text":"<p>The Spectral file field is an import component for the Performance evaluation dataset and allows import of spectral data in mzML format into the dashboard. Either a single mzML file may be imported, or a compressed archive (<code>zip</code> or <code>tar.gz</code> archive) of a single mzML file may be imported. Due to the often large size of the file, it is recommended to compress it prior to import.</p> <p>More information on how to generate valid mzML files is provided here.</p> <p>Note</p> <p>Meta-PepView performs validation checks to confirm that the spectral data and metaproteomics data were generated from the same experiment. However, due to inconsistencies in reporting of the mass spectometry run between input files, validation of the run is performed based on the name of the raw spectral file (file name without the extension). Since some metaproteomics files only report the name of the mzML file used as input, it is strongly recommended to ensure that the mzML file has the same name as the raw spectral file used as input for mzML generation (this is often the default behavior during mzML generation). Otherwise, meta-PepView may falsely invalidate combinations of spectral and metaproteomics datasets.</p>"},{"location":"data-import/#features","title":"Features","text":"<p>The Features field is an import component for the Performance evaluation dataset and allows import of feature data in featureXML format. The featureXML file contains a feature map dataset generated from the mzML spectral data. The feature map contains all features in an MS run. A feature is a 2D description of a peak that represents a single compound measured in the MS run. Features are generated by clustering spectral signals that are expected to belong to the same compound (due to isotope peaks or repeated measurements during the elution range). Inclusion of feature information allows further evaluation of the chromatographic profile of the MS experiment, as well as evaluation of contamination/noise.</p> <p>Meta-PepView supports import of a single featureXML file, or a compressed archive (<code>zip</code> or <code>tar.gz</code> archive) of a single featureXML file. More information on how to generate featureXML files is provided here.</p>"},{"location":"data-import/#taxonomy-annotation","title":"Taxonomy annotation","text":"<p>The Taxonomy annotation field is an import component for the Compositions and functions dataset and is used for supplementing taxonomic information to the peptides provided in the metaproteomics import data. The main source for taxonomic annotation is from a user supplied accession-taxonomy mapping file (often referred to as local annotation). Meta-PepView supports import of GhostKOALA format output data (for expected format, see here), or a user generated accession-taxonomy map file. </p> <p>For user generated taxonomy files, they should contain an accession column, which may be protein IDs linked to peptide sequences from the DB search dataset or peptide sequences directly. These accessions are mapped with a taxonomy ID, which should be stored in a separate column. Meta-PepView supports annotation of NCBI or GTDB format taxonomies. Since the dataset matches protein IDs to taxonomy, this source will only annotate DB search data. More information about the expected format of the protein-taxonomy map is found here.</p> <p>Meta-PepView provides an extensive menu with options how it should parse the accession-taxonomy map dataset: </p> <ul> <li>Delimiter: The user may set the expected delimiter for the tabular data format.</li> <li>Accession type: Specify if accessions are protein IDs, or peptide sequence. (Meta-PepView will automatically parse the correct data from the DB search dataasets.)</li> <li>Accession parser: Specify how the accession values, either as complete string or by extracting a substring. It should only be used for protein ID accessions. This can be used to match the accession (protein ID) format to those reported in DB search data.</li> <li>Pattern match (regex): Set a regex pattern to match a pattern group of the accession column (Only if Accession parser is set to \"Custom regex\").</li> <li>Accession/Taxonomy column index: Set column indices (starting at 0) for accession and taxonomy values. </li> <li>Taxonomy element format: Specify if the taxonomy column contains taxonomy IDs or taxonomy names.</li> <li>To NCBI taxonomy ID (genome IDs only): (GTDB ONLY) For a dataset of genome IDs present in the GTDB database, convert genomes to a NCBI taxonomy ID, then process data with NCBI taxonomy format.</li> </ul> <p>Meta-PepView can collect taxonomy information by querying the peptide sequences directly to Unipept (is enabled by the Annotate peptides to Unipept switch at the top of the sample import box). This allows taxonomy annotation of both DB search data and de novo data, and does not require any additional dataset to be imported. In Unipept, peptide sequences are matched against the UniprotKB protein database, and taxonomy information is collected from matching proteins. Meta-PepView requests the Last Common Ancestor for each peptide sequence. </p> <p>The taxonomy information fetched from Unipept is stored separately from the local annotated taxonomy information. Therefore, both strategies may be performed simultaneously for a single sample. This is useful when the local taxonomy map is validated in the Evaluate community composition module. It may also be used to supplement the local taxonomy annotation data in the Community composition module.</p> <p>Note</p> <p>Unipept only supports taxonomy annotation in NCBI format, it cannot be used as an extra taxonomy annotation strategy for GTDB format taxonomy.</p> <p>Warning</p> <p>It is not recommended to convert GTDB taxonomy to NCBI format. The conversion may result in a loss of taxonomy information, may decrease the resolution of the taxonomy annotation, and in the worst case, cause inaccurate taxonomy classifications. Use this only if the sample needs to be in NCBI format and there is no other way to obtain NCBI format taxonomy annotations.</p>"},{"location":"data-import/#functional-annotation","title":"Functional annotation","text":"<p>The Functional annotation field is an import component for the Compositions and functions dataset and is used for supplementing protein functional information to the peptides provided in the metaproteomics input data (only DB search). Meta-PepView supports processing and visualization of functional information in KEGG orthology format. Meta-PepView supports import of functional annotation data from two sources:</p> <ul> <li>EggNOG</li> <li>GhostKOALA</li> </ul> <p>More information about expected format of functional annotation data can be found here.</p> <p>Both tools perform functional annotation of proteins based on a protein sequence database provided as fasta. Therefore, functional information is mapped to protein IDs, which have to be matched to the protein IDs linked to the peptide sequences inside the DB search files. At the moment, functional annotation is only possible for DB search matched peptide data.</p> <p>For both datasets, only the KEGG Orthology (KO) identifier is of interest for the meta-PepView dashboard. However, EggNOG provides annotations for proteins from several databases (Pfam, COG, gene ontology, etc.). These are preserved when exporting the project table in CSV format and can thus be analyzed manually.</p> <p>Note</p> <p>GhostKOALA and EggNOG both use their own databases to functionally annotate proteins. As a result, proteins might be differently annotated depending on what tool is being used. If a particularly complex sample is being analyzed, it might be worthwhile to experiment with both tools to observe the effect on the output.</p> <p>Options for configuration of functional annotation data is limited. Similarly to DB search and taxonomy import, a protein ID pattern may be specified to extract from the protein ID fields in the functional annotation dataset to make it compatible with Protein ID report formats from DB search data. </p> <p>In addition, there is the option, Combine multiple annotations, that may be checked. When this option is checked, it will combine functional annotations for peptides that have multiple protein matches: For each field it will get all variants and concatenate them into a new value. When unchecked, it will discard functional annotations of peptides if a conflict is found.</p>"},{"location":"data-import/#import-experiment-into-performance-evaluation-dataset","title":"Import experiment into performance evaluation dataset","text":"<p>Import of an experiment into the performance evaluation dataset is straightforward. A spectral dataset (mzML) is required to import an experiment, while the other components (features, DB search, de novo) are optional. However, the full experiment performance evaluation module will only be visualized when all components are supplied. After all input sources are imported into the dashboard, the experiment data can be processed and incorporated into the project by pressing Add sample.</p>"},{"location":"data-import/#importing-a-reference-dataset-of-multiple-experiments","title":"Importing a reference dataset of multiple experiments","text":"<p>The experiment performance evaluation module includes cross-experiment comparison and benchmarking of experiments by import of a experiment reference dataset. This is a dataset that contains performance metrics for a large set of experiments. Some pre-generated datasets are included into the module for quick testing, but custom reference datasets for import into the module may be created using the <code>mpv-buildref</code> tool.</p>"},{"location":"data-import/#import-samples-into-compositions-and-functions-dataset","title":"Import samples into compositions and functions dataset","text":""},{"location":"data-import/#important-consider-the-input-format-constraints","title":"IMPORTANT: Consider the input format constraints","text":"<p>When combining multiple samples into a single sample table, meta-PepView is strict in managing import formats that is allowed. When data is imported into an existing project, meta-PepView performs the following checks if a sample is compatible for import:</p> <ul> <li>All samples should be compatible in taxonomy and function characterization. Thus, a project can only contain samples with either NCBI or GTDB format taxonomies, not both.</li> <li>For each data component, all samples in a project must come from the same data format (e.g. Sage, Novor, GhostKOALA). The output of a sample may differ significantly based on the used input source, to ensure consistency across samples, meta-PepView allows no mixing of input sources. The input formats that the project follows and allows for new data can be seen on the top bar of the Create project page.</li> </ul>"},{"location":"data-import/#input-data-components","title":"Input data components","text":"<p>A sample consists of the following import components:</p> <ul> <li>DB search metaproteomics datasets</li> <li>De novo metaproteomics datasets</li> <li>A taxonomy annotation dataset (and/or taxonomy annotation by sequence matching in Unipept)</li> <li>A functional annotation dataset</li> </ul>"},{"location":"data-import/#import-multiple-db-searchde-novo-files-at-once","title":"Import multiple DB search/de novo files at once","text":"<p>For DB search and de novo metaproteomics input data, multiple files may be selected for import. However, only one taxonomy annotation and functional annotation file may be imported per sample. When multiple DB search or de novo metaproteomics files are selected, meta-PepView will either merge the datasets into a single sample, or generate a separate sample for each DB search file. This option is specified in the merge DB search files toggle. Merging multiple DB search and de novo files allows for easily combining fractions or replicates from a single experimental sample.</p>"},{"location":"data-import/#allowed-combinations-of-input-data-components","title":"Allowed combinations of input data components","text":"<p>Meta-PepView allows flexible inclusion or omission of input data, it will process all data supplied by the user and ignore processing tasks if a required data source is absent. However, a new sample should contain at least two import components:</p> <ul> <li>One metaproteomics dataset (peptides from DB search or de novo identification).</li> <li>One annotation source (taxonomy annotation or functional annotation).</li> </ul> <p>Some import components may be omitted if no data is present. However, this may limit the visualization tools that are available for the sample. In general, the input data configuration depends on the presence or absence of DB search input data.</p>"},{"location":"data-import/#import-sample-with-db-search-input","title":"Import sample with DB search input","text":"<p>When one (or multiple) DB search input datasets are provided, a sample may be imported if at least one of the taxonomy annotation or functional annotation source is provided. The taxonomy annotation source may be a local accession to taxonomy dataset, annotation of peptide sequences by Unipept, or both. One or multiple de novo datasets may be supplemented to the sample.</p>"},{"location":"data-import/#import-sample-without-db-search-input","title":"Import sample without DB search input","text":"<p>With only de novo metaproteomics input data in a sample, meta-PepView can still perform taxonomy analysis on the sample. To import a sample with only de novo data, simply check the checkbox Annotate peptides to Unipept. Meta-PepView can currently only perform taxonomy annotation of peptides by sequence matching in Unipept. No local taxonomy annotation file or functional annotation dataset can be added.</p>"},{"location":"data-import/#import-multiple-samples-at-once","title":"Import multiple samples at once","text":"<p>Meta-PepView allows import of a collection of metaproteomics experiments at once. This is done by loading multiple DB search datasets into the DB search field and unchecking the merge DB search files toggle. Note that this requires all DB search files to be compatible with the same taxonomy and functional database. In addition, sample names are automatically named after the DB search file names. Thus, these should be named accordingly. This option may be useful for time series analysis or separate import of replicates.</p> <p>When de novo metaproteomics data is included together with DB search data. Meta-PepView will internally match each de novo dataset with the corresponding DB search dataset through their common spectral raw file names. This ensures that they belong to the same mass spectrometry analysis. De novo files that do not match to any DB search dataset are ignored.</p> <p>Note</p> <p>Matching de novo metaproteomics data to DB search data can get complicated when single DB search data files or de novo data files contain multiple raw source files (multiple MS runs merged into one file). Currently, meta-PepView will merge all de novo files that share at least one raw source file with the DB search file. Though, this may result in redundancy where one de novo peptide identification may be added to multiple samples.</p>"},{"location":"evaluate-taxonomy/","title":"Evaluate community composition","text":"<p>The number of peptides identified from a MS experiment and the accuracy of the taxonomy composition depends greatly on the quality of the protein sequence database used for DB search matching and taxonomy mapping; peptides not present in the database will not be matched and species not present in the database will not be represented in the community composition. A potential indicator for a bad protein database is a low number of DB search matches, especially when combined with many confident de novo peptides. However, the number of DB search matches considered acceptable depends strongly on the type of sample. In addition, other experimental factors may influence the number of DB search matches.</p> <p>One method to evaluate the quality/coverage of the protein sequence database used for DB search matching and taxonomy annotation is by comparing taxonomy composition derived from the protein database, to the taxonomy composition obtained from Unipept annotation of de novo peptide sequences. Using this method, we can evaluate absence of proteins/taxonomies from the protein sequence database. </p>"},{"location":"evaluate-taxonomy/#the-community-composition-evaluation-module","title":"The Community composition evaluation module","text":"<p>Meta-PepView provides a separate module for this evaluation: Community composition evaluation. Its interface is similar to the Community composition module interface, except here, only one sample is visualized at a time. To visualize a sample in this module, both DB search and de novo metaproteomics data has to be supplied for the sample, and taxonomy annotation from the local accession-taxonomy map, as well as Unipept (check Annotate peptides to Unipept in Taxonomy annotation options) has to be performed.</p> <p></p> <p>On the right side, the top chart shows the taxonomy compositions for a single sample: The left bar shows the composition from DB search matches, annotated with the user provided taxonomy map. The right bar shows the taxonomy composition as annotated by Unipept. This may be from DB search + de novo peptides or from de novo peptides only (see Unipept composition de novo only setting). The bottom chart (Abundance ratio) shows over- and underrepresentation of taxonomy groups between the taxonomy compositions.</p> <p>The abundance ratio chart calculates for each taxonomy group the \"hyperbolic tangent of the log ratio\":  $$ abundance\\ ratio=tanh(log(x_{de\\ novo}) - log(x_{db\\ search})) $$ </p> <p>Here, \\(x\\) represents the fraction of the taxonomy group in either the DB search or de novo composition. The ratio is capped between a range of \\([-1, 1]\\), where -1 means a taxonomy group is only observed in DB search composition, +1 means the taxonomy group is only observed in de novo composition, and 0 means that the taxonomy group has identical relative abundance between the compositions. Thus, a higher number implies a larger fraction of a taxonomy group in the de novo composition relative to the DB search composition.</p> <p>Similarly to the Community composition module, there are several options to modify the visualization:</p> <ul> <li>Select sample: Select the sample of the project to display (only valid samples that have all data necessary to visualize are shown).</li> <li>Select taxa: Select specific taxonomies to display (Only if Taxa display type is set to Custom taxa)</li> <li>Display taxa: Specify what taxonomies to display: Top 10/23 abundant taxonomies, or user specified Custom taxa (Taxa provided in Select taxa)</li> <li>Display rank: Select taxonomy level to display.</li> <li>Unipept composition de novo only: For the de novo (Unipept) composition, filter out any peptide sequences that were also matched with DB search from the abundance calculation.</li> <li>Quantification: Quantify taxonomy groups as summed PSM counts or as summed signal intensities.</li> <li>Normalize abundances: Check to display taxonomies as fraction of total composition per sample.</li> <li>Include unannotated: Add peptides without annotation (at specified taxonomy rank) as separate \"Undefined\" category in the composition.</li> </ul>"},{"location":"experiment-evaluation/","title":"Spectral data analysis and experiment performance evaluation","text":"<p>Meta-PepView provides a separate module for visualization of (meta)proteomics experimental performance parameters, including comparison and benchmarking of experiments against a dataset of metaproteomics experiments. This visualization toolbox is present in the Experiment performance evaluation module. This module uses the performance evaluation dataset, loaded into the project, which contains spectral data and metaproteomics data from a single sample. This allows visualization of spectral quality parameters, as well as to link these parameters to peptide identification rates.</p> <p></p> <p>The Quality control module is structured in several tabs. The Experiment comparison tab provides visualizations to compare (benchmark) an imported experiment with a experimental reference dataset, the Experimental quality tab provides visualizations connecting spectral quality to peptide identification performance.</p>"},{"location":"experiment-evaluation/#experiment-comparison","title":"Experiment comparison","text":"<p>The experiment comparison tab provides a set of visualizations that compare a single experiment against a dataset of experiments. Meta-PepView provides a few standard datasets, shown under the Reference dataset dropdown menu, for testing purposes. However, it is good practice to generate your own reference datasets with suitable samples and MS conditions. </p> <p>A new reference dataset can be imported from the import custom dataset importer. A CLI tool \"mpv-buildref\" is provided to build a new reference dataset. When providing a root directory (which should contain all mzML files, together with DB search and de novo data), as well as format settings, this tool will generate a summary file that contains performance parameters for all experiments that can be imported into the module.</p> <p>Note</p> <p>To prevent biases in proteomics data processing between samples, meta-PepView  will only compare experiments against the reference dataset if the DB search and de novo input data are in the same format for both dataset. The formats for the reference dataset is displayed after import into the dashboard.</p> <p>This format constraint is also present for the reference dataset. If a custom dataset is constructed, all DB search and de novo files should be in the same format across all experiments.</p> <p></p> <p>An overview of the available evaluation graphs is shown below.</p>"},{"location":"experiment-evaluation/#experiment-performance-metrics","title":"Experiment performance metrics","text":"<p>On top of the page, general performance metrics are displayed for all experiments in the reference dataset. These include: MS analysis time, number of MS1 and MS2 scans, number of MS2 scans per MS1 scan, as well as the number of DB search and de novo identifications. If a separate experiment is imported in the performance evaluation dataset, it will be overlayed in the graphs to compare against the reference dataset.</p> <p></p>"},{"location":"experiment-evaluation/#confidence-thresholds-barplot","title":"Confidence thresholds barplot","text":"<p>Quick profiling of the output quality from a (meta)proteomics experiment can be performed by comparing the number (or fraction) of peptide identifications above predefined confidence thresholds. The thresholds from a single experiment (left bar) can be compared to the experiments from the reference dataset (right bars). The graph allows scaling of the y-axis by peptide counts, fraction matches above thresholds of total scans, and the average number of matches above thresholds per minute (scaled by total retention time). For de novo identifications (bottom bars), either all identifications may be shown, or they may be filtered to only show de novo peptides not identified from DB search.</p> <p>While the graph allows quick observation of identification performance between experiments, certain patterns may imply issues in the experimental/analysis conditions:</p> <ul> <li>A low fraction of high confidence peptide identifications compared to lower confidences may imply lower spectral quality from the raw MS data. This should also be reflected in a low number of de novo identifications.</li> <li>A low number of DB search matches combined with a large number of (high confidence) de novo identifications may be due to the use of a database for DB search that does not represent the metaproteome of the sample well. Also, presence of high confidence de novo only peptides (peptides that were not identified from DB search matching) implies that.</li> </ul> <p>Note</p> <p>The numbers/fractions of DB search matches and de novo identifications that may be considered acceptable vary greatly between the type of samples and analysis conditions. Therefore, it is important that the reference dataset comprise experiments similar to the analysed sample.</p> <p></p>"},{"location":"experiment-evaluation/#peptide-confidence-scatterplot","title":"Peptide confidence scatterplot","text":"<p>An alternative method to observe the number of peptide identifications of an experiment (red markers) against the reference dataset (blue dots) is through a scatterplot. This graph shows the number (or fraction) of peptide identifications above predefined thresholds from DB search matching (left groups), de novo identifications (middle groups), and de novo only (d-only) identifications (right groups). Peptide identifications may be normalized by fraction of total MS2 scans, or by total retention time (count per minute).</p> <p></p>"},{"location":"experiment-evaluation/#confidence-ranked-distribution","title":"Confidence ranked distribution","text":"<p>The distribution of peptide identification confidence between an experiment (red line) and the reference dataset (gray area for 95% confidence interval) can be observed in the ranked distribution plot. Here, peptide identifications are sorted on the x-axis by confidence (y-axis). The distribution (x-axis) can be shown as top-n matches, or normalized as fraction of total matches, or fraction of MS2 matches. The distribution of either DB search or de novo identifications may be plotted.</p> <p>Note</p> <p>DB search and de novo tools often only report peptides above a confidence threshold, unless x-axis normalization is set to Total matches, the distribution of the reference dataset will be skewed as more experiments reach the confidence threshold (the end of the dataset). This should be taken into account when analysing the further end of the x-axis.</p> <p></p>"},{"location":"experiment-evaluation/#scan-intensity-distribution","title":"Scan intensity distribution","text":"<p>One metric for spectral quality evaluation is the distribution of MS scan intensities. Here, this is plotted as the median, 90'th percentile, and 99'th percentile intensity of MS1 and MS2 scan. An experimental sample (red marker) may be compared against the reference dataset (blue dots). </p> <p>Several factors may influence the intensity distribution of scans. For example, low MS1 intensities may imply that low amount of sample was submitted to the MS. Low MS2 intensity (relative to MS1) may imply low ion transmission efficiency (large loss of signal during fragmentation). Finally, strange patterns between median and x percentile groups may indicate presence of pollutants, or sparse elution of peptides. </p> <p></p>"},{"location":"experiment-evaluation/#ion-transmission","title":"Ion Transmission","text":"<p>Presence of pollutants during MS analysis or insufficient cleaning of the MS may result in greater loss of signal during ion transmission, especially during fragmentation of peptides prior to MS2 analysis. This may greatly impact the quality of (meta)proteomics data as MS2 signal loss lowers the confidence of peptide identification.</p> <p>Here, the distribution of ion transmission efficiencies of MS2 scans is visualized as total MS2 signal as percentage of the precursor ion measured in the prior MS1 scan (intensity of the peak from MS1, which was selected for MS2 scanning):</p> \\[ Eff_{ion\\ tr} = \\frac{Int_{tic\\ MS2}}{Int_{prec\\ MS1}} \\times 100\\% \\] <p>The median efficiency, as well as the 90'th and 99'th percentile (lowest efficiencies) are visualized. A high percentage indicates higher transmission efficiency. Thus, a larger fraction of total signal was measured in MS2.</p> <p>The transmission efficiency may be rescaled by ion injection time; a major factor of signal intensity is the accumulation time of ions prior to the MS scan, where shorter injection times result in lower signal intensities. This time may change on a per-scan basis to prevent oversaturation of the sensors. Generally, the injection time of MS1 scans are much shorter than those from MS2 scans (which may result in transmission efficiencies &gt;100%). In addition, MS1 injection times may vary strongly. By applying Scale ion injection time, all scans are corrected for their injection times before the ion transmission is calculated:  $$ Eff_{ion\\ tr} = \\frac{Int_{tic\\ MS2}}{Int_{prec\\ MS1}} \\times \\frac{Inj_{MS1}}{Inj_{MS2}} \\times 100\\% $$  </p> <p></p>"},{"location":"experiment-evaluation/#miscleavage-distribution","title":"Miscleavage distribution","text":"<p>Prior to MS submission, proteins are cleaved into short peptides, often with Trypsin (in combination with Lys-C) enzyme. These cleave proteins into peptides that have lengths well suited for MS detection and identification. This protein digestion is usually a time consuming step, and often, a balance is made between the amount of cleavage and digestion time. However, too short digestion time, as well as potential bad conditions during digestion, may result in a large fraction of multiply miscleaved peptides. As a result, a large fraction of protein material may become inaccessible for MS detection.</p> <p>This graph analyses DB search data from the reference dataset and the experiment dataset to provide a distribution of miscleavages, it counts the number of cleavage amino acids (arginine/lysine) present in each peptide (Except for the C-terminal amino-acid) and counts PSM's at each miscleavage number. These are provided as stacked bar graphs.</p> <p></p>"},{"location":"experiment-evaluation/#experimental-quality","title":"Experimental quality","text":"<p>The experimental quality page shows detailed information of spectral and (meta)proteomics performance for a single sample.</p> <p></p>"},{"location":"experiment-evaluation/#general-experiment-metrics","title":"General experiment metrics","text":"<p>Key metrics from a single MS experiment are displayed at the top of the page. These metrics give a quick overview of the experiment. These include:</p> <ul> <li>Total retention time (min): Total length of the MS run.</li> <li>MS1 scans: The number of MS1 (precursor) scans.</li> <li>MS2 scans: The number of MS2 (fragment) scans. Also shown as multiplier of MS1 scans.</li> <li>Features: The number of features captured in the featureXML file.</li> <li>Feature intensity (Frac. TIC): The combined signal intensity attributed to a feature, as fraction of the total intensity of all peaks.</li> <li>DB search matches: The number of DB search matches (PSM's). Also as fraction of MS2 (success rate).</li> <li>De novo identifications: The number of de novo peptides. Also as fraction of MS2 (success rate).</li> </ul> <p></p>"},{"location":"experiment-evaluation/#total-ion-current","title":"Total Ion Current","text":"<p>Show a profile of the total ion current (TIC) (and other parameters) over the retention time (RT). There are several options to change the data to display, or to combine spectral data with DB search matching de novo peptide identification data:</p> <ul> <li>MS level: Profile either MS1 TIC or MS2 TIC over retention time.</li> <li>SMA window: Smoothening of the line plot is done by applying the Simple Moving Average (SMA) to the scan TIC values. The SMA window is the number of raw MSx scans to take for a single SMA value.</li> <li>Data reduction factor: In case of large scan numbers, the data reduction factor can be set to reduce the number of data points by taking each n'th data point while omitting the other points. This loses information but speeds up operation of the figure.</li> <li>Right axis param: Overlay the TIC/RT line with a second parameter using the right y-axis:<ul> <li>DB search counts: Plot the number of DB search matches as histogram.</li> <li>De novo counts: Plot the number of de novo identifications as histogram.</li> <li>Peak count: Plot a simple moving average of the number of peaks per MS1/MS2 scan.</li> <li>Peak width (FWHM): Plot a simple moving average of elution times (at Full Width Half Maximum) of MS1 features.</li> <li>Feature quality: Plot a simple moving average of the calculated feature quality.</li> <li>Ion injection time: Plot a SMA of the MS1/MS2 ion injection time.</li> <li>topN MS2: Plot a SMA of the topN MS2 scans: The number of MS2 scans followed after a MS1 scan.</li> </ul> </li> <li>Confidence cutoff: (DB search counts de novo counts only) Only display DB search de novo peptide counts above the specified confidence threshold (only present if DB search de novo confidence selected as secondary parameter).</li> <li>int. cutoff: (Peak count only) Only consider peaks above a intensity threshold for visualization of the peak counts (see secondary param).</li> </ul> <p></p>"},{"location":"experiment-evaluation/#mz-over-rt","title":"m/z over RT","text":"<p>Plot MS2 scans by their precursor m/z value and the retention time. This allows for a detailed profiling of the elution profile and can be used to evaluate the used LC gradient. Scans may be grouped by successful DB search de novo identification and filtered by a specified MS2 intensity cutoff.</p> <p></p>"},{"location":"experiment-evaluation/#ms2-ion-transmission","title":"MS2 Ion Transmission","text":"<p>Display the ion transmission efficiency for each MS2 scan in the spectral dataset. The MS2 Total Ion Current (TIC) (y-axis) is plotted against the precursor intensity (x-axis). Scans may be grouped by successful DB search de novo identification and a m/z cutoff may be applied to filter low m/z noise from the MS2 TIC.</p> <p></p>"},{"location":"experiment-evaluation/#scan-intensities","title":"Scan Intensities","text":"<p>The Scan Intensities figure shows the distribution of scans over the Total Ion Current range (TIC). The number of MSx scans at different TIC bins are counted. For MS2, scans may be grouped by successful DB search matched scans, de novo only identified scans (no DB search match), and unidentified scans. In addition, by checking normalize bars, the fraction of each scan category (color) at each TIC bin is displayed as opposed to the counts. </p> <p></p>"},{"location":"experiment-evaluation/#charge-distribution","title":"Charge distribution","text":"<p>A potential measure for contamination inside the MS run is the distribution of feature charges. Tryptic peptides will typically have charge 2-4 in the MS1 spectra. Therefore, a large fraction of high charged features (&gt;8) or singly charged features may be attributed to contamination in the sample. In this plot, the distribution of charge numbers across all observed features is displayed. These features may be partitioned in DB search matched, de novo only identified or unidentified fractions, if DB search and/or de novo data is included. This allows visualization of the success rate of peptide identification at a given charge state:</p> <p></p>"},{"location":"experiment-performances-biases/","title":"Evaluation of experiment performances and biases","text":"<p>Meta-PepView provides a number of tools to perform evaluation and benchmarking of metaproteomics experiments. Two modules are dedicated to evaluation of metaproteomics experiments:</p> <ul> <li>The Experiment performance evaluation module: Visualization and benchmarking of experimental performance parameters from spectral data (mzML) + proteomics data. This module processes and visualizes the performance evaluation dataset stored in the project.</li> <li>The Community composition evaluation module: Validate database quality used in DB search matching by comparing taxonomy composition from DB search matching + local db annotation against taxonomy composition from peptides (DB search and de novo) annotated by Unipept. Similarly to the Community composition module, this module visualizes the compositions and functions dataset stored in the project.</li> </ul> <p> Experiment evaluation modules highlighted in the sidebar</p> <p>In addition to dedicated experiment evaluation modules, meta-PepView provides a tool for investigation potential over- or underrepresentation of taxa in community compositions: the Taxonomic drop-off. This tool is part of the Community composition module.</p>"},{"location":"function-visualization/","title":"Community function visualization","text":"<p>Meta-PepView provides the community functions module for visualization of protein functions. This module is available for all samples for which functional annotation data is supplemented from EggNOG or KEGG format. The module quantifies KEGG terms from multiple samples for comparative functional analysis. In addition, the KEGG Pathway and BRITE database is integrated in the dashboard for browsing and visualization of various types of functional groups.</p> <p></p> <p>The quantification of KEGG terms between samples is visualized in a grouped barplot. All samples in the project for which a functional annotation source was provided during import are shown. When opening the module, initially, the top 20 most abundant (averaged over all samples) KEGG KO terms, quantified by PSM counts, are displayed. Left from the graph, a range of options is provided to visualize specific functional groups, display different grouping types, and to scale the quantifications:</p> <ul> <li>KEGG ID display format: Format of the functional groups to display per sample (categories on x-axis).</li> <li>Select groups: For displaying only a specific functional group, filter by:<ul> <li>Pathway: display KEGG terms belonging to certain pathway, selection from Select pathway and/or Select module menu's</li> <li>BRITE: display KEGG terms belonging to BRITE hierarchy group, selection from Select BRITE group menu.</li> <li>Manual: Manually select KEGG terms to display.</li> </ul> </li> <li>Quantification: Quantify function groups as summed PSM counts or as summed signal intensities.</li> <li>Normalize sample: Select sample as normalization factor: For all samples, function expression is quantified as over-/underrepresentation factor for the selected sample.</li> <li>Combine multiple annotations: For peptides with multiple annotations, if checked, combine annotation names into a separate category. If unchecked, count peptide abundance towards each functional element separately. For example: <code>Pept_A</code> has KEGG KO annotation: <code>K00370,K07306,K17050</code>. When the option is checked, <code>K00370,K07306,K17050</code> becomes a separate function group (separate from <code>K00370</code> or any other separate element). If unchecked, the abundance of <code>Pept_A</code> is counted towards all of <code>K00370</code>, <code>K07306</code>, <code>K17050</code>.</li> </ul> <p>Note</p> <p>When displaying KO terms, peptides may only have multiple annotations if multiple proteins were matched. When EC or Module terms are displayed, a single KO may match to multiple EC or Module terms. After translation of KO term to EC or Module term('s), meta-PepView will combine them in the same method when Combine multiple annotations is checked. However, in cases of peptides with multiple KO terms (due to multiple protein matches), meta-PepView will ensure that a peptide sequence will not be counted double within a single EC or Module term. Therefore, while redundancy is present when Combine multiple annotations is unchecked due to potential counting a single peptide towards multiple groups, peptides will only be counted once within a single term.</p> <ul> <li>Include taxonomies: Supplement functional annotation chart with taxonomy information. This shows which taxa contribute to the functional term expression. Taxonomy names can be displayed by hovering over the bars in the chart.</li> <li>Fractional abundances: Quantify functional terms as fraction of the total expressed functional terms.</li> <li>Clade filter: Filter expressed functional terms to only those from a specified taxonomy group; Specify a Root taxonomy group belonging to the selected Clade rank level, and only terms from that taxonomy are displayed.</li> <li>Export complete functions: Export all KEGG terms present in the dataset with the filter and quantification options applied.</li> <li>Visualize pathway map: Export KEGG KO terms to a pathway map provided by KEGG. Select samples to visualize in the map in Select Samples and export the data to the map with the Show pathway map button. This directs you to the KEGG website. Thus, an internet connection is required.</li> </ul>"},{"location":"getting-started/","title":"Getting started","text":"<p>When the meta-PepView server is running, the interface is accessed through the web browser from the URL: <code>http://localhost:8050</code>. When starting a new session, the dashboard will display the Create project page.</p> <p> Dashboard interface at startup</p> <p>The dashboard contains several modules for data management and visualization. These are accessed in the sidebar. The dashboard contains the following modules:</p> <ul> <li>Project management:<ul> <li>Create project: Manage project data by importing samples from metaproteomics experiments.</li> </ul> </li> <li>Community analysis:<ul> <li>Community composition: Visualize community taxonomy composition across samples from the project table.</li> <li>Community functions: Visualize functional expression quantification across samples from the project table.</li> </ul> </li> <li>Experiment evaluation:<ul> <li>Experiment performance evaluation: Visualization toolbox for evaluation of metaproteomics experiment performance metrics, as well as benchmarking of experiments against an experiment reference dataset.</li> <li>Community composition evaluation: Compare community taxonomy composition from one sample between peptide annotation from local protein database matching and peptide sequence matching against Uniprot TrEMBL through the Unipept API.</li> </ul> </li> </ul>"},{"location":"getting-started/#download-public-databases","title":"Download public databases","text":"<p>For import of metaproteomics data and visualization of taxonomy and function groups, meta-PepView requires access to public taxonomy and function databases. These have to be downloaded from their sources. To infer lineages from taxonomy annotations and parse the taxonomy tree, the NCBI or GTDB taxonomy datasets are required (depending on the selected taxonomy annotation format). For visualization of function groups, the KEGG function mapping dataset has to be fetched online.</p> <p>The presence status of public databases is shown at the bottom of the sidebar. From there, a menu can be accessed to automatically download the required datasets for processing and visualization in the dashboard.</p> <p>Note</p> <p>When downloading datasets, meta-PepView will create a directory <code>.metapepview</code> inside the user home directory. The directory path that meta-PepView will look for and write datasets can be changed on the top of the download menu. Note that meta-PepView will only create directories for each dataset when the user checks the Create parent dir boxes. Make sure to check these boxes when downloading the datasets for the first time.</p> <p> Public database download menu</p> <p>Note</p> <p>If meta-PepView is running inside a Docker container, downloaded databases will only persist during the lifetime of the container. To ensure that databases will be preserved across container instances, the storage directory can be mounted to a directory on the host computer or a volume using the <code>-v</code> flag when creating a new container: <pre><code>docker run -p 8050:8050 -v \"ref_db_volume:/home/.metapepview\" metapepview\n</code></pre></p>"},{"location":"getting-started/#create-a-new-project","title":"Create a new project","text":"<p>Once the public databases are imported, a new project can be created by importing metaproteomics datasets. A project contains all the data for community analysis and experiment evaluation. These are stored in two datasets: </p> <ul> <li>A \"Performance evaluation dataset\" that contains raw spectral data and peptide identification data (DB search, de novo) from a single metaproteomics experiment. This dataset is visualized in the experiment performance evaluation module.</li> <li>A \"Compositions and functions dataset\" that contains peptide identifications (DB search, de novo) with taxonomy and function annotations from multiple experiments. This dataset is visualized in the taxonomy and functional visualization modules. </li> </ul> <p>New samples are added to a project by importing them in the \"Data importers\" field. Before importing data into a project, check out the Prepare input data section for supported data formats and sources, as well as instructions on how to obtain the data. Instructions to import samples into the project is provided in the Data import section.</p> <p> Import metaproteomics experiment data into a project</p>"},{"location":"installation/","title":"Installation","text":"<p>Meta-PepView runs on top of the Dash framework in python. It is installed as a web server that is run locally on the PC and accessed inside the web browser. Before installation, check with the PC requirements if the system is suitable for running the dashboard tool.</p> <p>Meta-PepView can be installed from <code>pip</code>/<code>pipx</code> or by starting a Docker container. </p>"},{"location":"installation/#install-meta-pepview-with-pipx-or-pip","title":"Install meta-PepView with <code>pipx</code> (or <code>pip</code>)","text":"<p>The full meta-PepView application (including the mpv-buildref command line utility) can be installed on any PC system with <code>pip</code>. This installation provides the meta-PepView dashboard, as well as a command line utility for construction of experimental reference datasets. </p> <p>It is recommended to install meta-PepView with pipx. It works similar to <code>pip</code>, but is designed for installation of applications as opposed to libraries. When installing a package, <code>pipx</code> provides a separate environment for the tool to run without cluttering existing python environments and ensures the executables are exposed to the command line. When using <code>pip</code>, it is recommended to install meta-PepView in a separate environment (virtual environment or conda)</p> <p>Note</p> <p>In the instructions below, meta-PepView is installed by pip(x) by fetching the code from the GitHub repository. For this, git needs to be installed on the PC.</p>"},{"location":"installation/#installation-with-pipx","title":"Installation with <code>pipx</code>","text":"<p>First, make sure python (3.11 or higher) is installed. Once python is installed, install pipx on the command line/terminal.</p> <p>In Windows 10/11, <code>pipx</code> can be installed via <code>pip</code>:</p> <pre><code># If you installed python using Microsoft Store, replace `py` with `python3` in the next line.\npy -m pip install --user pipx\npython -m pipx ensurepath\n</code></pre> <p>In Linux, <code>pipx</code> is often provided by a distro's built-in package manager:</p> <p>Ubuntu: <pre><code>sudo apt install pipx\npipx ensurepath\n</code></pre></p> <p>Fedora: <pre><code>sudo dnf install pipx\npipx ensurepath\n</code></pre></p> <p>You may need to restart the terminal before you can run <code>pipx</code>.</p> <p>Once pipx is installed, meta-PepView can be installed: <pre><code>pipx install git+https://github.com/RamonZwaan/metapepview.git\n</code></pre></p> <p>Once installed, the meta-PepView server can be started by running the application in the command line: <pre><code>metapepview\n</code></pre></p> <p>The dashboard can be accessed in the web browser (URL: <code>http://localhost:8050</code>).</p> <p>To uninstall meta-PepView: <pre><code>pipx uninstall metapepview\n</code></pre></p>"},{"location":"installation/#installation-with-pip","title":"Installation with <code>pip</code>","text":"<p>When python (3.11 or higher) is installed, meta-PepView may be installed directly with <code>pip</code>. Installation via <code>pip</code> will add meta-PepView as package to the global python environment and it is not certain that the application executables are exposed on the command line. To provide some isolation, it  is recommended to install meta-PepView in a virtual environment or in a conda environment.</p> <p>To install meta-PepView with pip: <pre><code>pip install git+https://github.com/RamonZwaan/metapepview.git\n</code></pre></p> <p>If the python environment is exposed to the operating system PATH, the meta-PepView server can be started by running the application in the command line: <pre><code>metapepview\n</code></pre></p> <p>To uninstall metapepview: <pre><code>pip uninstall metapepview\n</code></pre></p>"},{"location":"installation/#run-meta-pepview-inside-a-docker-container","title":"Run meta-PepView inside a Docker container","text":"<p>To run the dashboard inside a container, make sure Docker (or another OCI compliant manager) is installed on the system.</p> <p>First, build a Docker image using the Dockerfile template from the meta-PepView GitHub:</p> <p>Note</p> <p>Make sure the Docker daemon is running during the command execution (start Docker desktop if installed).</p> <pre><code>$ docker build -t metapepview https://github.com/RamonZwaan/metapepview.git#main\n</code></pre> <p>Now a Docker image named \"metapepview\" is created on the host system. The image can be found in Docker desktop, under Images.</p> <p>Once the image is successfully built, start a container by running the following command:</p> <pre><code>$ docker run -p 8050:8050 metapepview\n</code></pre> <p>Here, we start a container from the image \"metapepview\". The options<code>-p</code> publishes port 8050 from the container to port 8050 in the host pc, which allows us to reach the dashboard that is running inside the container. The dashboard can be accessed in the web browser at  <code>http://localhost:8050</code>. Note that the dashboard is only accessible from the host PC as long as the port is closed in the firewall.</p>"},{"location":"prepare-input-data/","title":"Preparing input data for meta-PepView","text":"<p>Meta-PepView combines metaproteomics data (DB search, de novo), raw spectral data (mzML, featureXML), and protein annotation data (taxonomy, functional) from several sources to provide interactive microbial community visualization and experiment evaluation. This section provides detailed information how to obtain input data for processing with meta-PepView, as well as considerations for data formatting to correctly parse and combine input data.</p>"},{"location":"prepare-input-data/#important-when-importing-metaproteomics-and-annotation-data-for-community-analysis-always-consider-the-protein-id-format","title":"IMPORTANT: When importing metaproteomics and annotation data for community analysis, always consider the protein ID format.","text":"<p>Meta-PepView adds taxonomic and functional information to identified peptides (from DB search matching) by linking their IDs to taxonomy and function IDs from user-provided annotation files. This ID may be the peptide sequence string, or the protein ID from the sequence database used in DB search matching (which has to be provided in all files). The protein ID is often derived from the headers of a fasta file.</p> <p>A major difficulty in data processing is maintaining consistency in protein ID extraction from the fasta header across input file sources (DB search, taxonomy, function annotation files). This is especially apparent in standardized fasta header formats.</p> <p>For example, the fasta header of the Uniprot entry for an alcohol dehydrogenase in yeast looks like this:</p> <pre><code>&gt;sp|P00331|ADH2_YEAST Alcohol dehydrogenase 2 OS=Saccharomyces cerevisiae (strain ATCC 204508 / S288c) OX=559292 GN=ADH2 PE=1 SV=3\n</code></pre> <p>The official Uniprot ID for this protein is <code>P00331</code>. Some data processing sources are informed of the Uniprot header format and will identify this protein correctly as <code>P00331</code>. However, other sources may take a general approach to get the protein ID and extract up to the first white-space, resulting in a different ID: <code>sp|P00331|ADH2_YEAST</code>. As a result, information cannot be linked between two sources that use different extraction rules. </p>"},{"location":"prepare-input-data/#how-to-process-protein-ids","title":"How to process protein IDs","text":"<p>It is important to ensure that the protein ID is extracted in the same way for all data processing sources (metaproteomics and annotation data). It is recommended to extract a protein ID from the fasta header up to the first white-space. In most cases, this retains the full identifier to discern all proteins. Additionally, a manual check of the datasets is recommended to ensure that protein ID formats match between sources.</p> <p>Finally, if protein IDs do not match between input data sources, meta-PepView provides an option to define custom regular expressions to extract substrings from the protein ID values.</p> <p>Example</p> <p>The Uniprot protein ID <code>P00331</code> from the reported ID <code>sp|P00331|ADH2_YEAST</code> may be extracted with a regular expression that extracts all the text between the two \"|\" characters: <code>(?&lt;=\\|).+(?=\\|)</code>. The expression '<code>(?&lt;=\\|)</code>' means: look behind the \"|\" character and extract the pattern that corresponds to the regex after the \"|\". The expression '<code>(?=\\|)</code>' means: look before the \"|\" character and extract the pattern that corresponds to the regex before it. Finally, the middle section '<code>.+</code>' means extract any one or more characters.</p>"},{"location":"prepare-input-data/#metaproteomics-data","title":"Metaproteomics data","text":"<p>Meta-PepView takes peptide identified scans as input for community processing. It supports direct import of peptide identification results from several DB search matching and de novo search tools. The general format of such datasets look like this:</p> Scan Peptide Confidence ... Protein ID (DB search only) 20739 KVEAATLK 87.3 ... Protein-A;Protein-B 20313 KVEAATLK 78.5 ... Protein-A;Protein-B 21165 KVEAATLK 54.9 ... Protein-A;Protein-B 13211 TSILDAIR 79.6 ... Protein-C 13587 TSILDAIR 70.2 ... Protein-C ... ... ... ... ... <p>Note</p> <p>Meta-PepView has been tested on Thermo Fisher Scientific Orbitrap Mass spectrometers, where acquisition of spectra has been done in Data-Dependent Acquisition (DDA) mode. Support of other MS systems or acquisition modes is not guaranteed. </p>"},{"location":"prepare-input-data/#import-db-search-matching-data","title":"Import DB search matching data","text":"<p>DB search matching engines identify peptide sequences from raw mass spectrometry data by matching MS2 spectra against theoretical spectra derived from a user supplied protein sequence database (as fasta). This results in a dataset of scans with matched peptide sequences, and also shows to what proteins match to the peptides. Through decoy strategies, the False Discovery Rate can be calculated and subsequently controlled in the output data. Many search engines do this internally, eliminating the need for pre-filtering during data processing.</p> <p>The following DB search engines are supported in meta-PepView:</p> <ul> <li>Sage: Meta-PepView supports direct import of the <code>results.sage.tsv</code> file generated by Sage.</li> <li>Peaks Studio 10/11: Meta-PepView supports direct import of the <code>DB search psm.csv</code> (Peaks Studio 10) or <code>db.psms.csv</code> (Peaks Studio 11) export data.</li> <li>MaxQuant: Meta-PepView supports direct import of <code>evidence.txt</code> file generated by MaxQuant</li> </ul>"},{"location":"prepare-input-data/#import-de-novo-identification-data","title":"Import de novo identification data","text":"<p>De novo search engines identify peptide sequences directly from raw mass spectrometry data without use of a protein sequence database. While it is easy to set up such an analysis as there is no need to prepare a sequence database, it also provides a peptide identification method that is not affected by potential biases that may emerge from the protein sequence database. On the other hand, a lack of spectrum matching decreases the confidence of identified peptide sequences, resulting in much fewer and potentially inaccurate peptide identifications. In addition, there is no robust decoy strategy to estimate the False Discovery Rate from the identified peptides.</p> <p>The following de novo identification engines are supported in meta-PepView:</p> <ul> <li>Peaks Studio 10/11: Meta-PepView supports direct import of the <code>de novo peptides.csv</code> (Peaks Studio 10) or <code>XXX.denovo.csv</code> (Peaks Studio 11) export data (<code>XXX</code> refers to the project name).</li> <li>Novor (SearchGUI): Meta-PepView supports direct import of <code>XXX.novor.csv</code> generated from the Novor engine in SearchGUI. NOTE: de novo sequencing reports from novor.cloud are a different format and is not supported inside meta-PepView as of yet.</li> <li>Casanovo: Meta-PepView supports direct import of <code>XXX.mztab</code> de novo results generated from Casanovo.</li> </ul>"},{"location":"prepare-input-data/#dashboard-features-per-input-format","title":"Dashboard features per input format","text":"<p>Depending on the input format, some dashboard features may not be present due to the absence of some data fields in the input. The following features may be absent depending on the input format:</p> <ul> <li>Signal intensity quantification: taxonomy or protein function abundance by combined signal intensity is not supported for some input data formats due to lack of intensity reporting in the results.</li> <li>Spectral evaluation: Novor, as provided in SearchGUI, does not correctly report scan numbers. As a result, peptide identifications cannot be linked to scans and some graphs in the Experimental quality module will not be displayed properly. </li> </ul> <p>Below is an overview of feature support for each input format:</p> Input format Intensity quantification Spectral evaluation Peaks Studio 10/11 MaxQuant Sage Novor Casanovo"},{"location":"prepare-input-data/#taxonomyfunction-annotation-data","title":"Taxonomy/Function annotation data","text":""},{"location":"prepare-input-data/#import-taxonomy-annotation-data","title":"Import taxonomy annotation data","text":"<p>Taxonomy annotation adds taxonomic information to the identified peptides by mapping their corresponding protein IDs or peptide sequence to taxonomy IDs (In meta-PepView, the protein ID/peptide sequence is referred to as Accession). Unfortunately, tools that provide taxonomy information to proteins in a well defined format are sparse. Generally, it is recommended to extract taxonomy information from Uniprot by performing alignment of the UniprotKB database to a local protein fasta (for example derived from a metagenome), or by downloading a protein fasta in Uniprot format. Detailed information how to generate your own taxonomy annotation is shown in Generating your own taxonomy annotation data.</p> <p>However, for quick and easy taxonomy annotation. Meta-PepView provides direct import of GhostKOALA taxonomy results. While it is designed for functional annotation, GhostKOALA also provides taxonomy information to proteins at the Genus level. However, as the KEGG database is smaller than Uniprot and NCBI, and taxonomy resolution is capped at Genus level. The taxonomy information provided from this dataset is limited and biases may be present in the composition.</p>"},{"location":"prepare-input-data/#taxonomy-annotation-from-ghostkoala","title":"Taxonomy annotation from GhostKOALA","text":"<p>The easiest method to provide taxonomy information is by directly importing GhostKOALA results. To import taxonomy information from GhostKOALA results, select the format \"GhostKOALA\" in the taxonomy annotation box. To generate the results, start a job in the GhostKOALA dashboard, using the same protein fasta file used in DB search matching. After the job is finished, the results can be downloaded from the results page. Meta-PepView expects the Taxonomy data results provided by GhostKOALA. This usually is stored in the file <code>user.out.top</code>.</p> <p> Results page after annotation job is finished. Download taxonomy and function information from the 'Taxonomy data' field. This downloads the <code>user.out.top</code> output file (compressed in zip archive).</p> <p>Note</p> <p>GhostKOALA adds <code>user:</code> as prefix before each accession ID. This is automatically filtered out by meta-PepView. Therefore, no regular expression pattern is needed to manage the <code>user:</code> prefix.</p> <p>This dataset includes both taxonomy and function information, and can therefore also be used as functional annotation dataset.</p>"},{"location":"prepare-input-data/#generating-your-own-taxonomy-annotation-data","title":"Generating your own taxonomy annotation data","text":"<p>To provide detailed taxonomy information for a microbial community, it is recommended to generate the protein-to-taxonomy-mapping yourself in GTDB or NCBI format. Here, meta-PepView expects a tabular text format (e.g. <code>csv</code>, <code>tsv</code>, etc.) that provides an accession column that matches the protein IDs/peptide sequences from the DB search dataset, and a NCBI or GTDB format taxonomy ID column:</p> Accession (Protein id) Taxonomy ID (NCBI) Protein-A 562 Protein-B 550 Protein-C 4932 ... ... <p>Alternatively:</p> Accession (Peptide sequence) Taxonomy ID (NCBI) KVEAATLK 562 TSILDAIR 550 ... ... <p>Ideally, the dataset should be non-redundant for protein IDs/peptide sequences, where every protein ID is only present once. However, many alignment tools provide top n reporting of matches, causing a single accession to be reported multiple times (often with different taxonomy IDs). If there is redundancy in accessions, meta-PepView will resolve the taxonomy ID for each accession internally. In case a single accession is linked to different taxonomy IDs, meta-PepView will compute the Last Common Ancestor taxonomy ID to which all reported taxonomies are descended from. However, if the dataset is large and has strong redundancy, this may take some time to process.  </p> <p>The standard format expected by meta-PepView is a <code>tsv</code> file (delimiter <code>\\t</code>) with Accession (protein ID/peptide sequence) as the first column (Accession column index <code>0</code>) and taxonomy ID as second column (Taxonomy column index <code>1</code>). However, meta-PepView provides several options to read differently formatted files, as shown in Data Import section.</p> <p>There are several options to customize the taxonomy annotation dataset. The accession may be set to protein ID or peptide sequence, column indices for accession and taxonomy may be customized, and a regex pattern may be given to take only a pattern from the accession (useful if a prefix needs to be removed).</p> <p>In addition, meta-PepView can process either taxonomy IDs, or taxonomy names in the input dataset. However, it is strongly recommended to match accessions to taxonomy IDs; In NCBI, one taxonomy ID may be assigned multiple taxonomy names (aliases), or multiple taxonomy IDs may share identical taxonomy names. On the other hand, a taxonomy ID will always represent a unique taxonomy group.</p>"},{"location":"prepare-input-data/#how-to-obtain-accession-taxonomy-map-dataset","title":"How to obtain accession-taxonomy map dataset","text":"<p>Taxonomy information from protein IDs may be obtained from the Uniprot database. Here, taxonomy IDs are easily linked to the protein ID from the fasta header:</p> <p>UniprotKB format: (Taxonomy ID in <code>OX=...</code> field) <pre><code>&gt;sp|P0A796|PFKA_ECOLI ATP-dependent 6-phosphofructokinase isozyme 1 OS=Escherichia coli (strain K12) OX=83333 GN=pfkA PE=1 SV=1\nMIKKIGVLTSGGDAPGMNAAIRGVVRSALTEGLEVMGIYDGYLGLYEDRMVQLDRYSVSD\nMINRGGTFLGSARFPEFRDENIRAVAIENLKKRGIDALVVIGGDGSYMGAMRLTEMGFPC\nIGLPGTIDNDIKGTDYTIGFFTALSTVVEAIDRLRDTSSSHQRISVVEVMGRYCGDLTLA\nAAIAGGCEFVVVPEVEFSREDLVNEIKAGIAKGKKHAIVAITEHMCDVDELAHFIEKETG\nRETRATVLGHIQRGGSPVPYDRILASRMGAYAIDLLLAGYGGRCVGIQNEQLVHHDIIDA\nIENMKRPFKGDWLDCAKKLY\n</code></pre></p> <p>Uniref format: (Taxonomy ID in <code>TaxID=...</code> field) <pre><code>&gt;UniRef100_A0A143Q7I6 ATP-dependent 6-phosphofructokinase isozyme 2 n=1 Tax=Rhodococcus sp. PBTS 1 TaxID=1653478 RepID=A0A143Q7I6_9NOCA\nMILTLTANPSMDRTVTLDAALQRGAVHRATTTTTDPGGKGVNVARVLTAAGRPCTAVLPG\nTGSDPLLGALGALGVRYHAVPTTGLARTNLTVSEPDGTTTKINEPGTALAPETVAGLTAS\nVRELAQRAQWVVLSGSVPPGVDAGWYGDLVAAVRETSARVAVDTSDAPLLALAAGFPRTA\nPDLIKPNAEELGQLTGRDGEVLEHAAAQGDPMPTVEAARILVDRGVGAVLATLGASGAVL\nVTATGAWFATPPPITPRSTVGAGDSSLAGYVLADLDGADGAGRLARAVAYGSAAAALPGT\nRLPTPTDVHVDAVPVRSLSLPGSSALARHTS\n</code></pre></p> <p>If DB search matching is done with a Uniprot/Uniref header formatted fasta file as protein sequence database, there are several strategies to obtain a protein ID-taxonomy map. If the fasta was exported from the Uniprot website, the same dataset can be exported in <code>tsv</code> format from the website. Make sure to select the protein ID (Entry) and the taxonomy ID (Organism (ID)).</p> <p>Alternatively, an accession-taxonomy-map dataset (using protein IDs) may created directly from the fasta. Below is an example to extract all protein ID-tax pairs from a fasta with a single command using the <code>sed</code> tool in Bash (Only available in Linux or macOS):</p> <pre><code># Converts UniprotKB formatted fasta to protein-taxonomy map tsv file\nsed -rn \"s/&gt;(\\S+).+OX=([0-9]+).*$/\\1\\t\\2/p\" &lt;uniprot_data.fasta &gt;./prot_to_tax.tsv\n</code></pre> <pre><code># Converts Uniref formatted fasta to protein-taxonomy map tsv file\nsed -rn \"s/&gt;(\\S+).+TaxID=([0-9]+).*$/\\1\\t\\2/p\" &lt;uniref_data.fasta &gt;./prot_to_tax.tsv\n</code></pre> <p>If a protein sequence database was derived from a metagenome, or from another unannotated source, the proteins should be aligned against the Uniprot/Uniref database first. It is recommended to perform alignment with Diamond against the full UniprotKB (Swiss-Prot + TrEMBL) or the full Uniref100 database. After annotation, Diamond can provide an output file that maps protein IDs to Uniprot/Uniref IDs. These can be converted into the accession-taxonomy map dataset for meta-PepView.</p> <p>Note</p> <p>To obtain the output from Diamond in the correct format, configure the output format with the <code>--outfmt/-f</code> flag (see Diamond documentation). For example, to get the protein ID and full fasta header (which contains the taxonomy), set output format to <code>--outfmt 6 qseqid stitle</code>. <code>qseqid</code> is the protein ID from the input fasta file, <code>stitle</code> is the full fasta header for the query DB (for example, UniprotKB/Uniref100).</p>"},{"location":"prepare-input-data/#taxonomy-annotation-in-gtdb-format","title":"Taxonomy annotation in GTDB format","text":"<p>The Genome Taxonomy Database provides an alternative phylogenetic tree to classify bacteria and archaea. While it does not cover the complete tree of life (Eukaryota are absent), it provides a more consistent dataset compared to NCBI taxonomy. For example, phylogenetic classification is based on sequence identity distances between organisms. Also, the full dataset consists only of sequences with genome representation.</p> <p>Providing GTDB taxonomy annotation to protein IDs is more challenging. While GTDB does provide a  full protein sequence dataset, the dataset is packaged as separate genome grouped fasta files. These have to be manually linked into a single protein sequence dataset. In addition, the proteins only provide NCBI genome IDs (For example: <code>GCA_034132005.1</code>) in the header. Meta-PepView is able to match these genome IDs to GTDB taxonomy IDs. Therefore, the GTDB protein fasta can be used as alignment database in Diamond. From the output, protein data is matched against the genome ID and a accession-taxonomy map can be constructed.</p>"},{"location":"prepare-input-data/#functional-annotation","title":"Functional annotation","text":"<p>The functional annotation dataset adds functional information to peptide sequences from DB search data. This can be used to quantify the expression of various functional protein groups from the dataset, or to export functional expression pathways. Meta-PepView uses the KEGG functional annotation format to assign functional information to peptides.</p> <p>Meta-PepView supports direct import of output from two common functional annotation tools: </p> <ul> <li>EggNOG: Meta-PepView supports direct import of the <code>XXX.emapper.annotations</code> output file (<code>XXX</code> refers to a user specified prefix).</li> <li>GhostKOALA: Meta-PepView supports direct import of the <code>user.out.top</code> output file.</li> </ul>"},{"location":"prepare-input-data/#spectral-datasets","title":"Spectral datasets","text":"<p>For evaluation of metaproteomics experiments, meta-PepView provides a module for visualization and analysis of spectral mass spectrometry data. For this, meta-PepView expects spectral data in mzML format.</p>"},{"location":"prepare-input-data/#spectral-data-mzml","title":"Spectral data (mzML)","text":"<p>mzML is a human and machine readable format that can be derived from several raw spectral data formats (from mass spectrometers by ThermoFisher, Bruker, etc.). It contains all the observed signals measured in the MS, several additional parameters and properties for each MS scan, and metadata related to the MS run. </p> <p>Note</p> <p>For spectral data analysis, meta-PepView is designed for analysis of Data-Dependent Acquisition (DDA) MS experiments, and has been tested on QE Orbitrap mass spectrometers. Any spectral data from DDA tandem MS experiments should work. However, Data-Independent Acquisition (DIA) experiments may not be processed properly in meta-PepView.</p> <p>The recommended method to generate mzML data is by using the tool MSConvert, provided by ProteoWizard. Peak picking on MS level 1 - 2 should be enabled for mzML generation. Peaks may be compressed with zlib.</p> <p>Note</p> <p>The browser meta-PepView is displayed in has a file size limit, above which no file import is possible. For Chromium browsers, this size is ~300 MB while Firefox allows ~500 MB sizes. To decrease the file size, meta-PepView can work with zip or tar.gz compressed files (mzML file should be the only file in the archive). This should allow import of large MS runs into the dashboard. In the case that the file size is still too large, the mzML file can be shrunk further by setting a peak threshold. By limiting the peak threshold to the 100-200 most intense peaks, great size reduction is reached with minimal loss of valuable information.</p>"},{"location":"prepare-input-data/#feature-data-featurexml","title":"Feature data (featureXML)","text":"<p>Several spectral quality metrics rely on the grouping of raw MS signals into features. Features cluster all peaks (all isotopes measured over subsequent scans) that represent a single molecule. Several properties may be retrieved from the feature including the elution time (Peak width), molecule charge, and feature intensity. For feature data, meta-PepView expects a separate dataset in parallel to the mzML data in featureXML format. Feature data in this format can be generated by the OpenMS platform.</p> <p>To generate a feature dataset, the FeatureFinderCentroided function provided by OpenMS is used. Installation of OpenMS for Windows/macOS/Linux is described in the OpenMS documentation. For Linux (Ubuntu), OpenMS can be easily installed from the apt package manager:</p> <pre><code>$ sudo apt install OpenMS\n</code></pre> <p>When OpenMS is installed, a feature map in featureXML format can be generated by running:</p> <pre><code>$ FeatureFinderCentroided -in example.mzML -out example.featureXML\n</code></pre>"},{"location":"project-table/","title":"The project data","text":"<p>Samples that are imported into the dashboard will be stored into the Project. A single project consists of two datasets: The Performance evaluation dataset, and the Compositions and functions dataset. The former dataset allows storing of spectral and metaproteomics data of only one sample, while the latter allows sequential import of multiple samples into the dataset. Projects may be exported and imported in the dashboard, this is managed from the top row in the Create project page. Detailed instructions on how to import new data into a project is provided here.</p> <p>The project data is displayed on the bottom of the Data Imports page. Here, an overview is provided on the imported samples into the project. The performance evaluation dataset shows the name of the experiment, as well as what components are supplied into the dataset. The Composition and functions dataset provides a table of sample. This table gives a general overview of the imported samples: The sample names, names of taxonomy and functional database files, and information about DB Search and de novo data import. In addition, samples may be removed from the project via the project table. </p> <p></p>"},{"location":"quantification/","title":"Taxonomy and function group quantification","text":""},{"location":"spectra-processing/","title":"Spectral data processing","text":""},{"location":"taxonomy-dropoff/","title":"Evaluation of taxonomy clade dropoffs","text":""},{"location":"taxonomy-visualization/","title":"Community composition visualization","text":"<p>Meta-PepView provides two modules for taxonomy visualization of microbial communities: Community composition and Evaluate community composition. Community composition is the main module for taxonomy composition analysis. It allows visualization of taxonomy composition and comparison of compositions between all samples within the project.</p> <p>Additional parts of the Community composition module, as well as the Evaluate community composition module, focus on evaluation of taxonomy composition correctness and identification of potential quantification biases.</p>"},{"location":"taxonomy-visualization/#community-composition-module","title":"Community composition module","text":"<p>The Community composition module visualizes taxonomy compositions for all samples in the project. Taxonomy abundances are quantified based on peptide spectrum match (PSM) counts or summed signal intensities provided from the metaproteomics data. In addition, there are a range of options to rescale/normalize abundances, to change the taxonomy level to visualize, and to zoom into a particular taxonomy group. Composition can be visualized either as stacked bar chart or heatmap format.</p> <p></p> <p>A range of options are provided to modify the visualization, these are acessed through the options menu left from the plot:</p> <ul> <li>Select taxa: Select specific taxonomies to display (Only if Taxa display type is set to Custom taxa)</li> <li>Display taxa: Specify what taxonomies to display: Top 10/20 abundant taxonomies, or user specified Custom taxa (Taxa provided in Select taxa)</li> <li>Display rank: Select taxonomy level to display.</li> <li>Quantification: Quantify taxonomy groups as summed PSM counts or as summed signal intensities.</li> <li>Normalize abundances: Check to display taxonomies as fraction of total composition per sample.</li> <li>Include unannotated: Add peptides without annotation (at specified taxonomy rank) as separate \"Undefined\" category in the composition.</li> <li>Include Unipept annotation: Supplement peptide sequences without taxonomy annotation from the user supplied taxonomy map, with potential Unipept matched taxonomy annotation. Requires the Annotate peptides to Unipept option to be checked for some samples in the Taxonomy annotation settings from the import module.</li> <li>Enable side-by-side plot: Add an additional plot (facet plot) of the compositions. Enabling this option gives separate quantification options for the facet plot. This allows direct evaluation of compositions from different quantification settings.</li> <li>Clade filter: Select a specific taxonomy group to visualize; Specify a Root taxonomy group belonging to the selected Clade rank level, and only \"offspring\" taxonomy groups belonging to the root taxonomy will be visualized. Note: Display rank should be at a lower level than the Clade rank.</li> <li>Export complete compositions: Download a CSV file of taxonomy abundances at every rank (level) from all samples for manual processing. Abundance format depends on selected options. </li> </ul> <p>An additional option, Export figure data, on the upper right of the page allows export of the data that is displayed in the figure.</p> <p>Info</p> <p>The main method how Allow global annotation supplements extra taxonomy information to the composition is by including taxonomy annotation from de novo identified peptides to the composition. If the user provided taxonomy map represents the protein accessions from metaproteomics DB search data well, (almost) all DB search peptides are expected to have at least some taxonomy annotation. Thus, taxonomy information can only be extended by including de novo identified peptides that were annotated from Unipept.</p> <p>Allow global annotation is usually not recommended to check for regular taxonomy profiling. As combination of DB search annotation and Unipept annotation with de novo peptides may introduce biases. Also, if DB search matching was done with good quality protein databases, this option is expected to have minimal effect on the composition (Except for a strong increase in \"Undefined\" fraction).</p> <p>Cases where this option may be useful is: Include samples to the taxonomy visualization for which no DB search data and/or taxonomy map data was provided, only Unipept annotation. In addition, for bad quality protein databases used for DB search matching, this option may provide a significant fraction of extra taxonomy information.</p>"},{"location":"taxonomy-visualization/#taxonomic-drop-off-profiling-peptide-uniqueness-in-taxonomy-group","title":"Taxonomic drop-off: Profiling peptide uniqueness in taxonomy group","text":"<p>Meta-PepView provides several tools to identify/evaluate potential biases in the observed community composition, these are mainly described in the Evaluate community composition section. However, one tool for evaluation of taxonomy bias is present in the Community composition module: the drop-off analysis.</p> <p>The taxonomy drop-off provides an estimate of peptide uniqueness for a taxonomy group relative to the total community. Peptide uniqueness has a large impact on the quantification of a taxonomy group, as only peptides unique to that group can be assigned to it and included in quantification. Non-unique peptides (assigned to multiple taxonomies) will be assigned to a higher-level Last Common Ancestor (LCA) taxonomy. Therefore, taxa with few unique peptide sequences will see only a small fraction of its proteome be counted towards its abundance. </p> <p>The fraction of unique peptides may differ greatly between taxonomy groups. For example, because of presence/absence of closely related taxa in the community. This can translate into a bias in observed taxonomy abundance, where taxa with fewer unique peptides will be underrepresented in the community. To estimate this bias, we can compare the peptide drop-off observed for the lineage of the taxonomy group to the average drop-off from the taxonomy root down to the rank (level) of the taxonomy group. For a single taxonomy group (and a single sample), we can observe this drop-off by clicking the taxonomy category (a single colored bar) in the stacked bar chart. Then, a drop-off chart will appear.</p> <p></p> <p>Here, we observe a stacked bar chart with bars at every rank of the lineage for the selected taxonomy group. At every rank, we observe the number of PSM's (or total Area) from the parent rank that are assigned to the taxonomy group in our lineage (blue bar), to other taxonomy groups in the same rank (orange bars, hover mouse over the bars for taxonomy names), and the number of PSM's that lack annotation at that rank (the drop-off) (red bar). The taxonomy drop-off for the lineage (Lineage drop in the interface) is calculated by combining the drop from parent to child rank for the complete lineage down to the selected taxonomy group with the following function. For the drop-off from root to the selected taxonomy:</p> \\[ drop_{lin} = 1 - \\prod_{n=root}^{tax\\ rank} (1 - drop_n) \\] <p>Here, \\(drop_n\\) is the fraction of PSM's without taxonomy annotation at rank \\(n\\) (the red bar), for which the parent rank is part of the lineage. The average community drop-off (from root to the rank of the selected taxonomy) is calculated with:</p> \\[ drop_{av} = \\frac{PSM_{tax\\ rank}}{PSM_{root}} \\] <p>A lineage drop-off that is higher than the community average drop-off implies that the taxonomy group is underrepresented in the community, while a lower lineage drop-off implies overrepresentation.</p>"}]}